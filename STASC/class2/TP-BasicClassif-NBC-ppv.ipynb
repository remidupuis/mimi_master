{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation de méthodes élémentaires pour la classification supervisée : Naive Bayes et classifieur par plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce TP, nous aurons besoin des modules Python ci-dessous, il vous faut donc évidemment exécuter cette première cellule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données [Vertebral Column](https://archive.ics.uci.edu/ml/datasets/Vertebral+Column) permet d'étudier les pathologies d'hernie discale et de Spondylolisthesis. Ces deux pathologies sont regroupées dans le jeu de données en une seule catégorie dite `Abnormale`. \n",
    "\n",
    "Il s'agit donc d'un problème de classification supervisée à deux classes :\n",
    "- Normale (NO) \n",
    "- Abnormale (AB)    \n",
    "\n",
    "avec 6 variables bio-mécaniques disponibles (features).\n",
    "\n",
    "L'objectif du TP est d'implémenter quelques méthodes simples de classification supervisée pour ce problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Télécharger le fichier column_2C.dat depuis le site de l'UCI à [cette adresse](https://archive.ics.uci.edu/ml/datasets/Vertebral+Column). \n",
    ">\n",
    "> On peut importer les données sous python par exemple avec la librairie [pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html). Vous pourrez au besoin consulter la documentation de la fonction [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). \n",
    "> \n",
    "> Le chemin donné dans la fonction `read_csv`est une chaîne de caractère qui spécifie le chemin complet vers le ficher sur votre machine. On peut aussi donner une adresse url si le fichier est disponible en ligne.\n",
    ">\n",
    "> Attention à la syntaxe pour les chemins sous Windows doit etre de la forme  `C:/truc/machin.csv`. \n",
    "> \n",
    "> Voir ce [blog](https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f) pour en savoir plus sur la \"manipulation des chemins\" sur des OS variés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= 'column_2C.dat'\n",
    "Vertebral = pd.read_csv(file_path,\n",
    "                          delim_whitespace= ',',\n",
    "                          header= None)\n",
    "Vertebral.columns = [\"pelvic_incidence\",\n",
    "                     \"pelvic_tilt\",\n",
    "                     \"lumbar_lordosis_angle\",\n",
    "                     \"sacral_slope\",\n",
    "                     \"pelvic_radius\",\n",
    "                     \"degree_spondylolisthesis\",\n",
    "                     \"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vérifier à l'aide des méthodes `.head()`  et `describe()` que les données sont bien importées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pelvic_incidence</th>\n",
       "      <th>pelvic_tilt</th>\n",
       "      <th>lumbar_lordosis_angle</th>\n",
       "      <th>sacral_slope</th>\n",
       "      <th>pelvic_radius</th>\n",
       "      <th>degree_spondylolisthesis</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.03</td>\n",
       "      <td>22.55</td>\n",
       "      <td>39.61</td>\n",
       "      <td>40.48</td>\n",
       "      <td>98.67</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>AB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.06</td>\n",
       "      <td>10.06</td>\n",
       "      <td>25.02</td>\n",
       "      <td>29.00</td>\n",
       "      <td>114.41</td>\n",
       "      <td>4.56</td>\n",
       "      <td>AB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.83</td>\n",
       "      <td>22.22</td>\n",
       "      <td>50.09</td>\n",
       "      <td>46.61</td>\n",
       "      <td>105.99</td>\n",
       "      <td>-3.53</td>\n",
       "      <td>AB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.30</td>\n",
       "      <td>24.65</td>\n",
       "      <td>44.31</td>\n",
       "      <td>44.64</td>\n",
       "      <td>101.87</td>\n",
       "      <td>11.21</td>\n",
       "      <td>AB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.71</td>\n",
       "      <td>9.65</td>\n",
       "      <td>28.32</td>\n",
       "      <td>40.06</td>\n",
       "      <td>108.17</td>\n",
       "      <td>7.92</td>\n",
       "      <td>AB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pelvic_incidence  pelvic_tilt  lumbar_lordosis_angle  sacral_slope  \\\n",
       "0             63.03        22.55                  39.61         40.48   \n",
       "1             39.06        10.06                  25.02         29.00   \n",
       "2             68.83        22.22                  50.09         46.61   \n",
       "3             69.30        24.65                  44.31         44.64   \n",
       "4             49.71         9.65                  28.32         40.06   \n",
       "\n",
       "   pelvic_radius  degree_spondylolisthesis class  \n",
       "0          98.67                     -0.25    AB  \n",
       "1         114.41                      4.56    AB  \n",
       "2         105.99                     -3.53    AB  \n",
       "3         101.87                     11.21    AB  \n",
       "4         108.17                      7.92    AB  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vertebral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pelvic_incidence</th>\n",
       "      <th>pelvic_tilt</th>\n",
       "      <th>lumbar_lordosis_angle</th>\n",
       "      <th>sacral_slope</th>\n",
       "      <th>pelvic_radius</th>\n",
       "      <th>degree_spondylolisthesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.496484</td>\n",
       "      <td>17.542903</td>\n",
       "      <td>51.930710</td>\n",
       "      <td>42.953871</td>\n",
       "      <td>117.920548</td>\n",
       "      <td>26.296742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.236109</td>\n",
       "      <td>10.008140</td>\n",
       "      <td>18.553766</td>\n",
       "      <td>13.422748</td>\n",
       "      <td>13.317629</td>\n",
       "      <td>37.558883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>26.150000</td>\n",
       "      <td>-6.550000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>70.080000</td>\n",
       "      <td>-11.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>46.432500</td>\n",
       "      <td>10.667500</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>33.347500</td>\n",
       "      <td>110.710000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>58.690000</td>\n",
       "      <td>16.360000</td>\n",
       "      <td>49.565000</td>\n",
       "      <td>42.405000</td>\n",
       "      <td>118.265000</td>\n",
       "      <td>11.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>72.880000</td>\n",
       "      <td>22.120000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>52.692500</td>\n",
       "      <td>125.467500</td>\n",
       "      <td>41.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>129.830000</td>\n",
       "      <td>49.430000</td>\n",
       "      <td>125.740000</td>\n",
       "      <td>121.430000</td>\n",
       "      <td>163.070000</td>\n",
       "      <td>418.540000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pelvic_incidence  pelvic_tilt  lumbar_lordosis_angle  sacral_slope  \\\n",
       "count        310.000000   310.000000             310.000000    310.000000   \n",
       "mean          60.496484    17.542903              51.930710     42.953871   \n",
       "std           17.236109    10.008140              18.553766     13.422748   \n",
       "min           26.150000    -6.550000              14.000000     13.370000   \n",
       "25%           46.432500    10.667500              37.000000     33.347500   \n",
       "50%           58.690000    16.360000              49.565000     42.405000   \n",
       "75%           72.880000    22.120000              63.000000     52.692500   \n",
       "max          129.830000    49.430000             125.740000    121.430000   \n",
       "\n",
       "       pelvic_radius  degree_spondylolisthesis  \n",
       "count     310.000000                310.000000  \n",
       "mean      117.920548                 26.296742  \n",
       "std        13.317629                 37.558883  \n",
       "min        70.080000                -11.060000  \n",
       "25%       110.710000                  1.600000  \n",
       "50%       118.265000                 11.765000  \n",
       "75%       125.467500                 41.285000  \n",
       "max       163.070000                418.540000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vertebral.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 310 entries, 0 to 309\n",
      "Data columns (total 7 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   pelvic_incidence          310 non-null    float64\n",
      " 1   pelvic_tilt               310 non-null    float64\n",
      " 2   lumbar_lordosis_angle     310 non-null    float64\n",
      " 3   sacral_slope              310 non-null    float64\n",
      " 4   pelvic_radius             310 non-null    float64\n",
      " 5   degree_spondylolisthesis  310 non-null    float64\n",
      " 6   class                     310 non-null    object \n",
      "dtypes: float64(6), object(1)\n",
      "memory usage: 17.1+ KB\n"
     ]
    }
   ],
   "source": [
    "Vertebral.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Les librairies de Machine Learning telles que `sckitlearn` prennent en entrée des tableau numpy (pas des objets pandas). Créer un tableau numpy que vous nommerez `VertebralVar` pour les features et un vecteur numpy `VertebralClas` pour la variable de classe. Voir par exemple [ici](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html#pandas.DataFrame.to_numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63.03  22.55  39.61  40.48  98.67  -0.25]\n",
      " [ 39.06  10.06  25.02  29.   114.41   4.56]\n",
      " [ 68.83  22.22  50.09  46.61 105.99  -3.53]\n",
      " ...\n",
      " [ 61.45  22.69  46.17  38.75 125.67  -2.71]\n",
      " [ 45.25   8.69  41.58  36.56 118.55   0.21]\n",
      " [ 33.84   5.07  36.64  28.77 123.95  -0.2 ]]\n",
      "['AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB' 'AB'\n",
      " 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO'\n",
      " 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO'\n",
      " 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO'\n",
      " 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO'\n",
      " 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO'\n",
      " 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO'\n",
      " 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO' 'NO'\n",
      " 'NO' 'NO']\n"
     ]
    }
   ],
   "source": [
    "VertebralVar  = Vertebral.drop(columns = 'class').to_numpy()\n",
    "VertebralClas = Vertebral['class'].to_numpy()\n",
    "\n",
    "print(VertebralVar)\n",
    "print(VertebralClas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage train / test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En apprentissage statistique, classiquement un prédicteur est ajusté sur une partie seulement des données et l'erreur de ce dernier est ensuite évaluée sur une autre partie des données disponibles. Ceci permet de ne pas utiliser les mêmes données pour ajuster et évaluer la qualité d'un prédicteur. Cette problématique est l'objet du prochain chapitre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En utilisant la fonction [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) de la librairie [`sklearn.model_selection`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection), sélectionner aléatoirement 60% des observations pour l'échantillon d'apprentissage et garder le reste pour l'échantillon de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "VertebralVar_train,VertebralVar_test,VertebralClas_train, VertebralClas_test = train_test_split(VertebralVar, VertebralClas, train_size = 0.6)\n",
    "ntot = len(VertebralClas) ### longueur totale de l'échantillon -  TO DO ####\n",
    "ntrain = len(VertebralClas_train) ### longueur totale de l'échantillon d'apprentissage - TO DO ####\n",
    "ntest = len(VertebralClas_test)### longueur totale de l'échantillon de test -TO DO ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : on peut aussi le faire à la main avec la fonction [`sklearn.utils.shuffle`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des deux classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Extraire les deux sous-échantillons de classes respectives \"Abnormale\" et \"Normale\" pour les données d'apprentissage et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 54.92  21.06  42.2   33.86 125.21   2.43]\n",
      " [ 78.49  22.18  60.    56.31 118.53  27.38]\n",
      " [ 72.34  16.42  59.87  55.92  70.08  12.07]\n",
      " [ 92.03  35.39  77.42  56.63 115.72  58.06]\n",
      " [ 77.41  29.4   63.23  48.01 118.45  93.56]\n",
      " [ 45.44   9.91  45.    35.54 163.07  20.32]\n",
      " [ 79.94  18.77  63.31  61.16 114.79  38.54]\n",
      " [ 63.36  20.02  67.5   43.34 131.    37.56]\n",
      " [ 44.55  21.93  26.79  22.62 111.07   2.65]\n",
      " [ 78.43  33.43  76.28  45.   138.55  77.16]\n",
      " [ 58.78   7.67  53.34  51.12  98.5   51.58]\n",
      " [ 43.72   9.81  52.    33.91  88.43  40.88]\n",
      " [ 80.99  36.84  86.96  44.14 141.09  85.87]\n",
      " [ 75.3   16.67  61.3   58.63 118.88  31.58]\n",
      " [ 70.48  12.49  62.42  57.99 114.19  56.9 ]\n",
      " [ 55.08  -3.76  56.    58.84 109.92  31.77]\n",
      " [ 31.48   7.83  24.28  23.66 113.83   4.39]\n",
      " [ 80.65  26.34  60.9   54.31 120.1   52.47]\n",
      " [ 46.44   8.4   29.04  38.05 115.48   2.05]\n",
      " [ 82.41  29.28  77.05  53.13 117.04  62.77]\n",
      " [ 95.48  46.55  59.    48.93  96.68  77.28]\n",
      " [ 54.12  26.65  35.33  27.47 121.45   1.57]\n",
      " [ 63.77  12.76  65.36  51.01  89.82  56.  ]\n",
      " [115.92  37.52  76.8   78.41 104.7   81.2 ]\n",
      " [ 35.49  11.7   15.59  23.79 106.94  -3.46]\n",
      " [ 65.01  27.6   50.95  37.41 116.58   7.02]\n",
      " [ 54.74  12.1   41.    42.65 117.64  40.38]\n",
      " [ 85.1   21.07  91.73  64.03 109.06  38.03]\n",
      " [ 57.29  15.15  64.    42.14 116.74  30.34]\n",
      " [ 31.23  17.72  15.5   13.52 120.06   0.5 ]\n",
      " [ 46.39  11.08  32.14  35.31  98.77   6.39]\n",
      " [ 63.4   14.12  48.14  49.29 111.92  31.78]\n",
      " [ 36.13  22.76  29.    13.37 115.58  -3.24]\n",
      " [ 55.51  20.1   44.    35.42 122.65  34.55]\n",
      " [ 63.07  24.41  54.    38.66 106.42  15.78]\n",
      " [ 59.6   32.    46.56  27.6  119.33   1.47]\n",
      " [ 60.63  20.6   64.54  40.03 117.23 104.86]\n",
      " [ 63.17   6.33  63.    56.84 110.64  42.61]\n",
      " [ 43.2   19.66  35.    23.54 124.85  -2.92]\n",
      " [ 64.27  12.51  68.7   51.77  95.25  39.41]\n",
      " [ 43.35   7.47  28.07  35.88 112.78   5.75]\n",
      " [ 57.04   0.35  49.2   56.69 103.05  52.17]\n",
      " [ 48.03   3.97  58.34  44.06 125.35  35.  ]\n",
      " [ 64.62  15.23  67.63  49.4   90.3   31.33]\n",
      " [ 76.31  41.93  93.28  34.38 132.27 101.22]\n",
      " [ 70.25  10.34  76.37  59.91 119.24  32.67]\n",
      " [ 56.56   8.96  52.58  47.6   98.78  50.7 ]\n",
      " [ 74.38  32.05  78.77  42.32 143.56  56.13]\n",
      " [ 36.69   5.01  41.95  31.68  84.24   0.66]\n",
      " [ 58.1   14.84  79.65  43.26 113.59  50.24]\n",
      " [ 52.42  19.01  35.87  33.41 116.56   1.69]\n",
      " [ 53.57  20.46  33.1   33.11 110.97   7.04]\n",
      " [ 57.52  33.65  50.91  23.88 140.98 148.75]\n",
      " [ 72.22  23.08  91.    49.14 137.74  56.8 ]\n",
      " [ 81.66  28.75  58.23  52.91 114.77  30.61]\n",
      " [ 76.33  42.4   57.2   33.93 124.27  50.13]\n",
      " [ 58.52  13.92  41.47  44.6  115.51  30.39]\n",
      " [ 76.15  21.94  82.96  54.21 123.93  10.43]\n",
      " [ 96.66  19.46  90.21  77.2  120.67  64.08]\n",
      " [ 77.12  30.35  77.48  46.77 110.61  82.09]\n",
      " [ 57.3   24.19  47.    33.11 116.81   5.77]\n",
      " [ 86.75  36.04  69.22  50.71 139.41 110.86]\n",
      " [ 67.26   7.19  51.7   60.07  97.8   42.14]\n",
      " [ 58.83  37.58 125.74  21.25 135.63 117.31]\n",
      " [ 60.42   5.27  59.81  55.15 109.03  30.27]\n",
      " [ 67.51  33.28  96.28  34.24 145.6   88.3 ]\n",
      " [ 55.84  28.85  47.69  27.   123.31   2.81]\n",
      " [ 86.47  40.3   61.14  46.17  97.4   55.75]\n",
      " [ 66.88  24.89  49.28  41.99 113.48  -2.01]\n",
      " [ 71.24   5.27  86.    65.97 110.7   38.26]\n",
      " [ 66.8   14.55  72.08  52.25  82.46  41.69]\n",
      " [ 71.19  23.9   43.7   47.29 119.86  27.28]\n",
      " [ 69.78  13.78  58.    56.   118.93  17.91]\n",
      " [ 83.7   20.27  77.11  63.43 125.48  69.28]\n",
      " [ 74.85  13.91  62.69  60.95 115.21  33.17]\n",
      " [ 39.06  10.06  25.02  29.   114.41   4.56]\n",
      " [ 77.24  16.74  49.78  60.5  110.69  39.79]\n",
      " [ 72.05  24.7   79.87  47.35 107.17  56.43]\n",
      " [ 89.5   48.9   72.    40.6  134.63 118.35]\n",
      " [ 45.37  10.76  29.04  34.61 117.27 -10.68]\n",
      " [ 48.11  14.93  35.56  33.18 124.06   7.95]\n",
      " [ 80.07  48.07  52.4   32.01 110.71  67.73]\n",
      " [ 85.35  15.84  71.67  69.51 124.42  76.02]\n",
      " [ 69.63  21.12  52.77  48.5  116.8   54.82]\n",
      " [ 74.01  21.12  57.38  52.88 120.21  74.56]\n",
      " [ 32.09   6.99  36.    25.1  132.26   6.41]\n",
      " [ 44.53   9.43  52.    35.1  134.71  29.11]\n",
      " [ 70.4   13.47  61.2   56.93 102.34  25.54]\n",
      " [ 50.91  23.02  47.    27.9  117.42  -2.53]\n",
      " [ 81.75  20.12  70.56  61.63 119.43  55.51]\n",
      " [ 26.15  10.76  14.    15.39 125.2  -10.09]\n",
      " [ 38.66  12.99  40.    25.68 124.91   2.7 ]\n",
      " [ 86.04  38.75  47.87  47.29 122.09  61.99]\n",
      " [ 55.29  20.44  34.    34.85 115.88   3.56]\n",
      " [ 41.77  17.9   20.03  23.87 118.36   2.06]\n",
      " [ 77.66  22.43  93.89  55.22 123.06  61.21]\n",
      " [ 86.9   32.93  47.79  53.97 135.08 101.72]\n",
      " [ 41.19   5.79  42.87  35.39 103.35  27.66]\n",
      " [ 49.78   6.47  53.    43.32 110.86  25.34]\n",
      " [ 60.04  14.31  58.04  45.73 105.13  30.41]\n",
      " [ 63.03  22.55  39.61  40.48  98.67  -0.25]\n",
      " [ 72.56  17.39  52.    55.18 119.19  32.11]\n",
      " [ 45.54  13.07  30.3   32.47 117.98  -4.99]\n",
      " [ 56.67  13.46  43.77  43.21  93.69  21.11]\n",
      " [ 58.6   -0.26  51.5   58.86 102.04  28.06]\n",
      " [ 65.01   9.84  57.74  55.18  94.74  49.7 ]\n",
      " [ 44.22   1.51  46.11  42.71 108.63  42.81]\n",
      " [ 75.65  19.34  64.15  56.31  95.9   69.55]\n",
      " [ 48.33  22.23  36.18  26.1  117.38   6.48]\n",
      " [ 40.25  13.92  25.12  26.33 130.33   2.23]\n",
      " [ 50.83   9.06  56.3   41.76  79.    23.04]\n",
      " [ 79.25  23.94  40.8   55.3   98.62  36.71]\n",
      " [ 37.9    4.48  24.71  33.42 157.85  33.61]\n",
      " [ 46.86  15.35  38.    31.5  116.25   1.66]\n",
      " [ 43.79  13.53  42.69  30.26 125.    13.29]\n",
      " [ 74.72  14.32  32.5   60.4  107.18  37.02]\n",
      " [ 41.73  12.25  30.12  29.48 116.59  -1.24]\n",
      " [ 81.1   24.79  77.89  56.31 151.84  65.21]\n",
      " [ 79.48  26.73  70.65  52.74 118.59  61.7 ]\n",
      " [ 88.02  39.84  81.77  48.18 116.6   56.77]\n",
      " [ 44.25   1.1   38.    43.15  98.27  23.91]\n",
      " [ 73.64   9.71  63.    63.92  98.73  26.98]\n",
      " [ 49.71   9.65  28.32  40.06 108.17   7.92]]\n"
     ]
    }
   ],
   "source": [
    "VertebralVar_train_AB = VertebralVar_train[VertebralClas_train == 'AB']\n",
    "VertebralVar_train_NO = VertebralVar_train[VertebralClas_train == 'NO']\n",
    "print(VertebralVar_train_AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "n_AB = len(VertebralVar_train_AB)\n",
    "n_NO = len(VertebralVar_train_NO)\n",
    "print(n_AB)\n",
    "print(n_NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ajuster un classifieur naif bayesien sur les données d'apprentissage.\n",
    "\n",
    "Pour une observation $x \\in \\mathbb R^6$, la régle du MAP consiste à choisir la catégorie $\\hat y (x) = \\hat k $ qui maximise (en $k$) \n",
    "$$ score_k(x) = \\hat \\pi_k \\prod_{j=1} ^6  \\hat f_{k,j}(x_j)   $$\n",
    "où :\n",
    "- $k$ est le numéro de la classe ;\n",
    "- $\\hat \\pi_k$ est la proportion observée de la classe $k$, \n",
    "- $\\hat f_{k,j} $ est la densité gaussienne univariée de la classe $k$ pour la variable $j$. Les paramètres de cette loi valent (ajustés par maximum de vraisemblance) :\n",
    "    - $\\hat \\mu_{k,j}$ : la moyenne empirique de la variable $X^j$ restreinte à la classe k,\n",
    "    - $ \\hat \\sigma^2_{k,j}$ : la variance empirique de la variable $X^j$ restreinte à la classe k.\n",
    "    \n",
    "Noter que la fonction $x \\mapsto  \\prod_{j=1} ^6  f_{k,j}(x_j) $ peut aussi être vue comme une densité gaussienne multidimensionnelle de moyenne $(\\mu_{k,1}, \\dots, \\mu_{k,6})$ et de matrice de covariance diagonale $diag(\\hat \\sigma^2_{k,1},\\dots,\\hat  \\sigma^2_{k,6})$. Cette remarque évite de devoir calculer le produit de 6 densités univariées, à la place on calcule plus directement la valeur de la densité multidimensionnelle.\n",
    "\n",
    "Pour calculer la valeur de la densité d'une gaussienne multidimensionnelle en un point $x$ de $\\mathbb R ^d$ on peut utililser la fonction [`multivariate_normal`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html) de la librairie [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html). \n",
    "\n",
    "On pourra utiliser la fonction `var` de numpy pour calculer le vecteur des variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul des moyennes et des variances de chaque variable pour chacun des deux groupes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_AB = VertebralVar_train_AB.mean(axis = 0) \n",
    "mean_NO = VertebralVar_train_NO.mean(axis = 0) \n",
    "\n",
    "# variances estimées variable par variable pour AB (sur le train) :\n",
    "var_AB = VertebralVar_train_AB.var(axis = 0) \n",
    "# variances estimées variable par variable pour NO (sur le train) :\n",
    "var_NO = VertebralVar_train_NO.var(axis = 0)\n",
    "\n",
    "# Les vraies matrices de covariance:\n",
    "vraie_Cov_NB_AB = np.cov(VertebralVar_train_AB, rowvar=False) \n",
    "vraie_Cov_NB_NO =  np.cov(VertebralVar_train_NO, rowvar=False) \n",
    "\n",
    "# on forme les matrices de covariance (matrices diagonales car indep) :\n",
    "Cov_NB_AB = np.diag(var_AB)\n",
    "Cov_NB_NO = np.diag(var_NO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul du \"score\" sur chaque groupe pour chaque element des données test : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AB', 'NO', 'NO', 'AB', 'AB', 'NO', 'NO', 'AB', 'AB', 'AB', 'NO',\n",
       "       'AB', 'AB', 'NO', 'AB', 'AB', 'AB', 'AB', 'AB', 'NO', 'AB', 'AB',\n",
       "       'AB', 'NO', 'AB', 'NO', 'AB', 'NO', 'AB', 'AB', 'NO', 'AB', 'AB',\n",
       "       'AB', 'NO', 'AB', 'NO', 'NO', 'NO', 'NO', 'NO', 'AB', 'AB', 'AB',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'AB', 'AB', 'NO', 'AB', 'AB', 'AB',\n",
       "       'AB', 'NO', 'AB', 'NO', 'NO', 'AB', 'AB', 'AB', 'NO', 'AB', 'NO',\n",
       "       'AB', 'NO', 'AB', 'AB', 'NO', 'AB', 'NO', 'NO', 'NO', 'NO', 'AB',\n",
       "       'NO', 'AB', 'NO', 'AB', 'AB', 'AB', 'AB', 'NO', 'AB', 'AB', 'AB',\n",
       "       'AB', 'AB', 'AB', 'NO', 'AB', 'AB', 'NO', 'AB', 'AB', 'AB', 'AB',\n",
       "       'AB', 'NO', 'NO', 'AB', 'NO', 'NO', 'NO', 'AB', 'AB', 'AB', 'AB',\n",
       "       'NO', 'AB', 'NO', 'NO', 'AB', 'AB', 'AB', 'AB', 'NO', 'NO', 'NO',\n",
       "       'NO', 'AB', 'AB'], dtype='<U2')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_AB = multivariate_normal(mean=mean_AB, cov=Cov_NB_AB)\n",
    "gnb_NO = multivariate_normal(mean=mean_NO, cov=Cov_NB_NO)\n",
    "\n",
    "likelihood_AB = gnb_AB.pdf(VertebralVar_test)\n",
    "likelihood_NO = gnb_NO.pdf(VertebralVar_test)\n",
    "\n",
    "prior_AB = n_AB / (n_AB + n_NO)\n",
    "prior_NO = 1 - prior_AB\n",
    "\n",
    "score_AB = likelihood_AB * prior_AB\n",
    "score_NO = likelihood_NO * prior_NO\n",
    "\n",
    "Vertebral_Clas_pred = np.where(score_AB > score_NO, 'AB', 'NO')\n",
    "\n",
    "Vertebral_Clas_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice de confusion est une matrice qui synthétise les performances d'une régle de classification. Chaque ligne correspond à une classe réelle, chaque colonne correspond à une classe estimée. La cellule (ligne L, colonne C) contient le nombre d'éléments de la classe réelle L qui ont été estimés comme appartenant à la classe C. Voir par exemple [ici](https://fr.wikipedia.org/wiki/Matrice_de_confusion).\n",
    "\n",
    "> Evaluer les performances de la méthode sur l'échantillon test. Vous pourrez utiliser la fonction [`confusion_matrix`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) de la librairie [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65, 22],\n",
       "       [ 8, 29]], dtype=int64)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix_test = confusion_matrix(y_true = VertebralClas_test, y_pred = Vertebral_Clas_pred)\n",
    "cnf_matrix_test#.astype('float') / cnf_matrix_test.sum(axis=1).reshape(-1,1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Il existe bien sûr une fonction scikit-learn  pour la méthode Naive Bayes : voir [ici](http://scikit-learn.org/stable/modules/naive_bayes.html). Vérifier que votre prédicteur donne la même réponse de cette fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65, 22],\n",
       "       [ 8, 29]], dtype=int64)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X = VertebralVar_train, y = VertebralClas_train)\n",
    "y_predicted = gnb.predict(X = VertebralVar_test)\n",
    "confusion_matrix(y_true = VertebralClas_test, y_pred = y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifieur par plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est préférable d'utiliser la structure de données de type [k-d tree](https://en.wikipedia.org/wiki/K-d_tree) pour effectuer des requêtes de plus proches voisins dans un nuage de points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Contruction du k-d tree pour les données train (pour la métrique euclidienne) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "tree =  ### TO DO ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rechercher les 10 plus proches voisins dans les données d'apprentissage du premier point des données de test et afficher les classes de ces observations voisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_voisins =  tree.query(### TO DO ####)\n",
    "print(indices_voisins)\n",
    "classes_voisins = ### TO DO ####\n",
    "print(classes_voisins)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le classifieur par plus proches vosins, la prediction est la classe majoritaire des k plus proches voisins.\n",
    "\n",
    "> Donner la prédiction pour le premier point de test par vote majoritaire sur ses 10 plus proches voisins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Donner la prediction du classifieur ppv pour toutes les données de test. Evaluer la qualité du classifieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_class = ### CHOISIR  ####  #nombre de plus proche voisins utilisés\n",
    "pred_kNN_test =  ### TO DO ####\n",
    "cnf_matrix_kNN =### TO DO ####\n",
    "cnf_matrix_kNN.astype('float') / cnf_matrix_kNN.sum(axis=1).reshape(-1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe bien sûr une fonction scikit-learn pour le classifieur plus proche voisin, voir [ici](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
